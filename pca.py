import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from sklearn import preprocessing
from matplotlib import pyplot as plt

# Load the dataset from the csv file in a pandas dataframe
# and extract an array of the values.
df = pd.read_csv('./data/VA_per90.csv')
d = df.values
#print(d)
#print(d.shape)
#print(df.head)

# Filter only the per90 columns. I can't run PCA on features like the
# ID of the players or on non-numerical attributes like the team or
# the position, I want principal components along the features
# that have a similar meaning to be able to understand something
# from the visualization.
per90_cols = [col for col in df.columns if col.endswith('_per90')]

# Even if I have all the statistics per 90 minutes to make them comparable
# and not depending only on the played minutes, I need to standardize 
# those features so that all of them contribute equally and all of them
# have the same scale.
scaler = preprocessing.StandardScaler()
x = scaler.fit_transform(df[per90_cols])
# I check mean (=0) and standard deviation (=1) to check normalization.
print("Mean: ", np.mean(x), " Std: ", np.std(x))

# Now I want to project the 46-dimensional (actually less since I ignored 
# some columns) data to 2-dimensional principal components.
pca = PCA(n_components=2)
x_pca = pca.fit_transform(x)

# I want to append the columns generated by the Principal Component
# a+Analysis to the original dataframe.
df['x'] = x_pca[:, 0]
df['y'] = x_pca[:, 1]

# I want to check the amount of variance that each principal component
# holds after I projected the data from almost 46 dimensions to only 2.
print('Explained variability per principal component: {}'.format(pca.explained_variance_ratio_))
# It prints: Explained variability per principal component: [0.13743094 0.10390414]
# This means that the principal component 1 holds almost 14% of the information,
# while principal component 2 holds a little more of 10%.
# It is not surprising to see that projecting data between a so high number of
# dimensions causes a lot of information loss: almost 76% of information
# is loss when we project data only on 2 dimensions.

###########################################################################

import matplotlib.pyplot as plt
plt.figure()
plt.figure(figsize=(10,10))
plt.xticks(fontsize=12)
plt.yticks(fontsize=14)
plt.xlabel('Principal Component - 1',fontsize=20)
plt.ylabel('Principal Component - 2',fontsize=20)
plt.title("Principal Component Analysis of VA_per90",fontsize=20)
plt.scatter(df['x'], df['y'], s = 10)
plt.show()


# Show the updated DataFrame with PCA coordinates
#df[['Player', 'Squad', 'market_value_in_eur', 'x', 'y']].head()


